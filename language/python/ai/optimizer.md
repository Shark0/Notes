# Optimizer

Optimizer（最佳化器）在機器學習和深度學習中是用來更新模型參數的演算法，目的是最小化損失函數（loss function），從而提高模型的預測準確性。

## Description 
### 作用
Optimizer的核心作用是通過調整模型的參數（如神經網路中的權重和偏差），使損失函數的數值逐漸減小，進而讓模型更好地擬合數據。具體來說：

* 最小化損失：損失函數衡量模型預測值與真實值之間的差距，Optimizer通過迭代更新參數來尋找損失函數的最低點。
* 加速收斂：Optimizer設計了高效的更新策略，幫助模型更快地找到損失函數的全局或局部最小值。
* 避免過擬合：一些最佳化器（如帶有正則化的Adam）能平衡模型的學習過程，減少過擬合風險。

### 工作原理

Optimizer通常基於梯度下降（Gradient Descent）或其變體，核心步驟如下：

* 計算梯度：根據損失函數對每個參數求偏導數，得到梯度，表示損失函數在當前參數下的變化方向。
* 更新參數：根據梯度和某種更新規則（如學習率）調整參數，使損失函數值減小。
* 迭代進行：重複計算梯度和更新參數，直到損失收斂或達到預設的訓練次數。

### 常見的Optimizer
以下是一些常見的最佳化器及其特點：

#### SGD（Stochastic Gradient Descent, 隨機梯度下降）
每次使用小批量（mini-batch）數據計算梯度並更新參數。
* 優點：簡單、計算成本低。
* 缺點：可能陷入局部最小值，收斂速度慢。

#### Momentum（動量法）
在SGD基礎上引入動量，考慮過去梯度的方向，加速收斂。
* 優點：能越過小型局部最小值，收斂更快。
* 缺點：需要調整動量參數。

#### Adam（Adaptive Moment Estimation）
結合動量法和RMSProp，通過自適應地調整學習率來加速梯度下降。
* 優點：收斂快，適用於大多數問題，參數調整需求少。
* 缺點：在大規模數據上可能過擬合。

#### RMSProp
通過對梯度平方進行指數加權平均，適應性地調整學習率。
* 優點：適合非穩定的損失函數。
* 缺點：需要手動設置學習率。

#### Adagrad
根據參數的歷史梯度自適應調整學習率，適合稀疏數據。
* 優點：無需手動調整學習率。
* 缺點：學習率可能過早變小，導致收斂緩慢。

### Optimizer的選擇
選擇合適的Optimizer取決於任務類型、數據規模和模型結構：

* 小型簡單模型：SGD或Momentum通常足夠。
* 深度學習模型：Adam因其高效性和穩定性是首選。
* 稀疏數據或特定任務：Adagrad或RMSProp可能更合適。
* 大規模分佈式訓練：需要考慮如LARS或LAMB等專為大規模訓練設計的Optimizer。

### 注意事項
* 學習率（Learning Rate）：學習率決定參數更新的步長，太大可能導致不收斂，太小則收斂過慢。許多Optimizer（如Adam）能自適應調整學習率。
* 數值穩定性：梯度爆炸或消失可能影響訓練，需通過梯度裁剪或正則化解決。
* 調參：不同Optimizer有不同超參數（如Adam的β1、β2），需根據實驗調整。


## Code Function
### Init 
```
optimizer = optim.Adam(model.parameters(), lr=lr)
```
這行代碼初始化了一個Adam 優化器，用於根據梯度更新模型的參數以最小化損失函數。

#### `optim.Adam`
* `Adam`（Adaptive Moment Estimation）是一種廣泛使用的優化算法，結合了動量法和 RMSProp 的優點，通過自適應地調整學習率來加速梯度下降。
* 它維護了梯度的一階動量（均值）和二階動量（方差的未中心化估計），使得參數更新更穩定且收斂更快。

#### `model.parameters()`
* `model` 是繼承自 `nn.Module`模型
* `model.parameters()` 返回模型中所有需要優化的參數（例如，Model的權重和偏置）。 
 
這些參數會被傳遞給優化器，優化器負責計算它們的梯度並更新值。

#### `lr=lr`
* `lr` 是學習率（learning rate），控制參數更新的步長。
* `lr=0.001` 是一個常見的初始值，表示每次參數更新時梯度的縮放比例。
學習率太高可能導致訓練不穩定，太低則收斂過慢。

### 梯度清零
```
optimizer.zero_grad()
```
將模型參數的梯度清零，以確保當前批次的梯度計算不會受到之前批次梯度的影響。

#### 梯度累積
* 在 PyTorch 中，當你調用 `loss.backward()` 計算梯度時，梯度會被累積到每個參數的 `.grad` 屬性中。
* 如果不清零，當前批次的梯度會與之前的梯度相加，導致錯誤的參數更新。

#### 為什麼需要清零
* 每個批次（batch）的梯度應該獨立計算，基於當前批次的損失。
* `optimizer.zero_grad()` 將所有參數的 `.grad` 設為 `None` 或零，確保梯度從頭開始計算。

### 更新模型的參數
```
optimizer.step()
```
根據計算的梯度更新模型的參數，執行一次優化步驟。

* 在調用 `loss.backward()` 後，PyTorch 會計算損失對每個參數的梯度，儲存在參數的 `.grad` 屬性中。
* `optimizer.step()` 使用這些梯度，按照優化算法（這裡是 Adam）的規則更新參數的值，更新後，模型的預測（正向分數高於負向分數的能力）會逐步改進。
* 對於 Adam 優化器，更新規則考慮了梯度的一階動量和二階動量，以及學習率。