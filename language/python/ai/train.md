# Train

## Train Scenario
* Supervised Learning: 要提供教材與標籤
* Semi Supervise: 給教材，但只給部分標籤
* Reinforce Learning: 只給分數

不同的應用、學習、資料情境要挑選不同的[模型架構](./neural_network.md#架構實作)、[Loss Function](loss_function.md)、[Optimizer](./optimizer.md)

## 訓練流程
1. 設置[Data](./data.md)
2. 設置[模型架構](./neural_network.md#架構實作)
3. 設置[Loss Function](loss_function.md)
4. 設置[Optimizer](./optimizer.md)

在每次訓練中，根據Loss值透過Optimizer來調整模型結構參數，讓下一次Loss值變小

## 相關參數
### Batch Size
#### 對訓練效率的影響
* 訓練速度：
  * 較大的 Batch Size：每個批次處理更多樣本，GPU/TPU 的並行計算能力得到更充分利用，訓練速度通常更快（每步迭代的時間更短）。
  * 較小的 Batch Size：每個批次處理的樣本數較少，計算資源利用率可能較低，導致訓練速度較慢。

* 記憶體需求：
  * 較大的 Batch Size：需要更多的 GPU/CPU 記憶體來儲存中間計算結果（例如梯度和激活值）。如果 Batch Size 過大，超出硬體記憶體限制，可能會導致記憶體溢出錯誤（OOM）。
  * 較小的 Batch Size：記憶體需求較低，適合記憶體受限的設備，但可能需要更多迭代次數來完成訓練。

#### 對模型性能的影響
* 梯度估計的穩定性：
  * 較大的 Batch Size：梯度估計更穩定，因為每個批次包含更多樣本，梯度代表了更廣泛的數據分佈。這有助於模型更平穩地收斂，特別是在數據分佈較均勻的情況下。
  * 較小的 Batch Size：梯度估計的隨機性更高，可能導致訓練過程中梯度更新方向波動較大。這可能使模型難以收斂，或者陷入次優解。但在某些情況下，這種隨機性也有助於模型跳出局部極小值，找到更好的解。

* 泛化能力：
  * 較小的 Batch Size：由於梯度更新更頻繁且隨機性更高，模型可能更好地適應數據中的細微模式，從而提高泛化能力（在驗證集或測試集上的表現更好）。
  * 較大的 Batch Size：可能導致模型過於依賴批次中的主要模式，減少對數據中細微變化的學習，從而可能降低泛化能力。

#### 對收斂速度的影響
* 較大的 Batch Size：由於梯度估計更穩定，模型可能在早期訓練階段收斂得更快，但可能需要更多的 epoch（輪次）來達到最佳性能。
* 較小的 Batch Size：訓練過程可能更不穩定，早期收斂速度較慢，但長期來看可能探索到更好的解。

#### 對 SentenceTransformer 特定任務的影響
SentenceTransformer 通常用於生成句子的嵌入（embedding），並在語義搜索、句子相似度計算、聚類等任務中應用。Batch Size 的選擇可能因具體任務而異：

* 語義相似度任務（如使用 MultipleNegativesRankingLoss）：
  * 較大的 Batch Size 有助於包含更多正樣例和負樣例對，增強模型對比學習的效果。例如，MultipleNegativesRankingLoss 需要在批次內進行負樣例挖掘，Batch Size 越大，負樣例的多樣性越高，可能提高模型的語義分辨能力。
  * 但如果 Batch Size 過大，模型可能過於關注批次內的局部模式，忽略全局數據分佈。

* 分類任務（如使用 CrossEntropyLoss）：
  * 較小的 Batch Size 可能有助於模型更好地學習數據中的細微差異，尤其是在數據不平衡的情況下。

* 數據集大小：
  * 如果數據集較小，較大的 Batch Size 可能導致模型過擬合，因為每個 epoch 的梯度更新次數較少。
  * 如果數據集較大，較大的 Batch Size 可以加速訓練，且對性能影響較小。

#### 如何選擇合適的 Batch Size
選擇 Batch Size 時需要權衡訓練速度、記憶體限制和模型性能：

* 硬體限制：檢查 GPU/CPU 的記憶體容量，選擇最大的 Batch Size，確保不超過記憶體限制。通常可以從較小的 Batch Size（例如 16 或 32）開始，逐步增加，直到接近記憶體上限。
* 任務需求：
  * 對比學習任務（如語義相似度）通常需要較大的 Batch Size（例如 64、128 或更高）以提供足夠的負樣例。
  * 分類任務可以嘗試較小的 Batch Size（例如 8、16 或 32）以提高泛化能力。

* 學習率調整：Batch Size 與學習率密切相關。較大的 Batch Size 通常需要較大的學習率來保持梯度更新的幅度。經驗法則是：如果 Batch Size 增加k倍，學習率也應適當增加
* 試驗與驗證：在實際應用中，建議通過交叉驗證或在驗證集上測試不同 Batch Size 的效果，選擇性能最佳的配置。

### Epoch
1 epoch = 模型完整看過 一次整個訓練集。

一般建議（依據任務）

| 任務類型                | 建議epoch範圍 | 說明                  |
|---------------------|-----------|---------------------|
| 小型文本分類 / 相似度	       | 3 ~ 10    | 通常 3~5 就夠，太多會過擬合    |
| 微調 BERT / SBERT     | 1 ~ 5     | 預訓練模型已學很多，1~3 通常就有效 |
| 大型分類（多類別）           | 5 ~ 20    | 視資料量與收斂情況調整         |
| 對比學習（如 TripletLoss） | 10 ~ 50+  | 收斂較慢，需較多 epoch      |
| 語言模型預訓練             | 10 ~ 100+ | 巨量資料，長時間訓練          |

### Warmup Step
warmup_steps 指的是訓練開始時的一段學習率預熱階段（Learning Rate Warmup）。在這段階段中，學習率從一個較小的值（通常為 0 或接近 0）逐漸線性增加到設定的最大學習率（由優化器指定的學習率）。
直觀理解： 預熱階段就像讓模型「熱身」，避免一開始就用過高的學習率進行大步參數更新，這樣可以讓模型更平穩地進入訓練狀態。


#### Warmup Steps 對訓練的影響

warmup_steps 通過控制早期訓練的學習率，影響模型的收斂速度、穩定性和最終性能。以下是具體影響：

##### 提高訓練穩定性
* 為什麼需要預熱？
  * 在訓練開始時，模型參數（權重）通常是隨機初始化的，或者從預訓練模型微調，這些參數可能遠離最優值。
  * 如果一開始就使用較高的學習率，可能導致梯度更新過大，參數變化劇烈，損失函數可能劇烈波動，甚至發散。

* 預熱的作用：
  * 通過從較小的學習率開始，模型可以進行小步更新，逐漸適應數據分佈和損失函數的形狀。
  * 這有助於避免訓練早期的不穩定性，特別是在使用較大的 Batch Size 或複雜的損失函數（如 MultipleNegativesRankingLoss）時。

##### 改善模型收斂
* 早期探索：
  * 較小的學習率允許模型在參數空間中進行更細粒度的探索，有助於找到更好的初始路徑，朝向全局或高質量的局部最小值。

* 避免過擬合早期模式：
  * 如果一開始學習率過高，模型可能過快地適應訓練數據中的某些模式（例如噪聲或異常值），導致過擬合或收斂到次優解。
  * 預熱階段讓模型有機會更全面地學習數據分佈，提升最終的收斂質量。

##### 與 Batch Size 的交互
* 大 Batch Size: 較大的 Batch Size 通常提供更穩定的梯度估計，但可能需要更高的學習率來確保更新幅度足夠。直接使用高學習率可能導致不穩定，而 warmup_steps 允許學習率逐漸增加，平衡穩定性和更新效率。

* 小 Batch Size： 小 Batch Size 的梯度隨機性較高，早期使用高學習率可能放大這種隨機性，導致訓練不穩定。預熱階段可以減輕這種影響。

##### 對損失函數的適應
* 在 SentenceTransformer 中，常用的損失函數（如 CosineSimilarityLoss 或 MultipleNegativesRankingLoss）涉及對比學習或相似度計算。這些損失函數在訓練初期可能對參數變化非常敏感。
* 預熱階段讓模型逐步適應損失函數的梯度分佈，避免因初始梯度過大而導致參數更新失控。

##### 對長訓練過程的影響
* 在長時間訓練（多個 epoch）中，warmup_steps 的設置影響學習率的整體調度
  * 如果 warmup_steps 過短，學習率可能過快達到最大值，導致早期訓練不穩定。 
  * 如果 warmup_steps 過長，模型可能在低學習率下花費過多時間，延遲收斂，降低訓練效率。

* 通常，warmup_steps 設置為總訓練步數的 10%~20% 是一個合理的經驗值，但具體值取決於任務和數據集。

#### 設置建議
* 數據集大小：
  * 對於較大的數據集，warmup_steps 可以設置為總步數的 10%~20%（總步數 = 數據集大小 / Batch Size × epochs）。 
  * 對於較小的數據集，warmup_steps 可以設置為較小的值（例如 50~500），以避免過長的預熱階段。

* 任務類型：
  * 對比學習任務（如語義相似度）可能需要較長的預熱階段，因為損失函數對初始參數變化敏感。
  * 分類任務可能需要較短的預熱，因為損失函數（如交叉熵）通常更穩定。

* 試驗與驗證：
  * 在實際應用中，嘗試不同的 warmup_steps（例如 100、500、1000），並監控訓練損失和驗證集性能，選擇最佳值。

* 與學習率調度結合：
  * SentenceTransformer 通常使用線性學習率衰減（Linear Decay with Warmup）。在預熱階段後，學習率會從最大值逐漸衰減到 0。確保 warmup_steps 與總訓練步數匹配，以充分利用學習率調度。

####  常見問題與解決方案
* 問題：訓練早期損失劇烈波動。
  * 解決方案：增加 warmup_steps 或降低最大學習率，確保早期更新更平穩。
  
* 問題：模型收斂太慢。
  * 解決方案：減少 warmup_steps 或提高最大學習率，加速參數更新。

* 問題：驗證集性能不佳。
  * 解決方案：檢查 warmup_steps 是否過長，導致模型在低學習率下學習不足；或者過短，導致早期過擬合。通過試驗調整。


### Early Stoping
Early Stopping（提前停止）是機器學習和深度學習中常用的一種正則化技術，用於防止模型在訓練過程中過擬合（overfitting），並節省計算資源。它的核心思想是在訓練過程中監控模型在驗證集上的性能，當性能不再提升（或開始下降）時，提前停止訓練。

#### Early Stopping 的工作原理
* 監控驗證集性能：
  * 在每個訓練週期（epoch）或指定步數後，使用驗證集（獨立於訓練集的數據）評估模型的性能
  * 常用的監控指標包括：
    * 損失函數值（如驗證損失 val_loss，例如均方誤差或交叉熵）
    * 評估指標（如驗證準確率 val_accuracy、F1 分數、餘弦相似度等）

* 設定停止條件：
  * 耐心值（Patience）：定義允許性能未提升的連續訓練週期數。例如，patience=5 表示如果連續 5 次驗證損失未降低，則停止訓練
  * 最小改進（min_delta）：指定性能提升的最小幅度，防止因微小波動而誤判。例如，min_delta=0.001 表示損失必須至少降低 0.001 才算進步

* 停止訓練
  * 如果連續 patience 次監控指標未改善（例如驗證損失未降低或準確率未提高），則終止訓練
  * 通常會恢復在驗證集上表現最佳的模型權重（即最佳檢查點）

* 保存最佳模型
  * 在訓練過程中記錄表現最好的模型參數，以便在停止後使用最佳模型

#### Early Stopping 的優點
* 防止過擬合： 當模型在訓練集上表現越來越好，但在驗證集上性能開始下降時，Early Stopping 能及時停止訓練，避免模型過分擬合訓練數據
* 節省計算資源: 無需運行完整設定的訓練週期（epochs），可以減少訓練時間和能耗
* 自動選擇最佳模型: 通過保存最佳模型權重，確保最終模型是在驗證集上表現最好的版本

#### Early Stopping 的局限性
* 過早停止的風險： 如果 patience 設置過小或驗證集數據不具代表性，可能在模型尚未達到最佳性能時停止訓練。
* 驗證集依賴： 需要一個獨立的驗證集。如果驗證集太小或不平衡，可能導致誤判。
* 指標選擇的影響： 選擇不合適的監控指標（例如只看損失而忽略準確率）可能影響結果。

#### 在什麼情況下使用 Early Stopping
* 深度學習任務：如圖像分類、自然語言處理（例如 Sentence Transformers）、語音識別等。
* 數據量較大：訓練時間長、計算成本高時，Early Stopping 能有效節省資源。
* 模型容易過擬合：特別是當模型複雜度高或訓練數據有限時。

### Dropout
Dropout 是一種在訓練神經網路（包括 SentenceTransformer 等語言模型）時常用的正則化技術，旨在防止過擬合（Overfitting），提高模型的泛化能力。
以下是對 Dropout 的詳細解釋，包括它的作用、原理以及在你的聊天機器人任務（小數據集、語義匹配）中的應用。

#### Dropout 是什麼
Dropout 在訓練過程中隨機「丟棄」（暫時禁用）神經網路中的一部分神經元（包括它們的輸入和輸出連接），並在每次前向傳播時以一定的概率（通常稱為 Dropout 率，例如 0.1 或 0.2）決定哪些神經元被丟棄。
在推斷（推理）階段，Dropout 會被關閉，所有神經元都參與計算，但輸出通常會進行縮放以保持一致性。

Dropout 就像在每次訓練迭代中隨機「關閉」一部分神經元，強迫模型依賴不同的神經元組合來學習，從而避免過分依賴某些特定神經元。

#### Dropout 的作用
Dropout 的主要作用是增強模型的泛化能力，具體影響如下

##### 防止過擬合

在小數據集（例如你的 100 多個樣本）上訓練神經網路時，模型容易過擬合，即在訓練數據上表現很好，但在未見數據（驗證集或測試集）上表現較差。

##### Dropout 的解決方案

通過隨機丟棄神經元，Dropout 減少了模型對訓練數據中特定模式的依賴，迫使模型學習更魯棒的特徵。 這相當於在訓練過程中模擬了多個「子網路」的集合，類似於集成學習（Ensemble Learning），提高了模型對新數據的適應能力。

##### 增加模型魯棒性
Dropout 使模型在不同神經元組合下都能正常工作，類似於讓模型「學會應對不確定性」。

這對於你的聊天機器人任務（將提問映射到連結）尤為重要，因為實際用戶的提問可能與訓練數據略有不同，Dropout 幫助模型更好地處理這種變異。

##### 減少共適應
沒有 Dropout 時，某些神經元可能會過於依賴其他神經元，形成特定的「共適應」關係，這可能導致模型過於專注於訓練數據的細節。

Dropout 打破這種共適應，強迫每個神經元獨立學習有用的特徵。

##### 提高訓練穩定性
Dropout 引入隨機性，減少了模型對初始參數或數據順序的敏感性，有助於穩定訓練過程。

##### 任務中的 Dropout 重要性
* 小數據集：你的數據集只有 100 多個樣本，過擬合風險高。Dropout 通過隨機丟棄神經元，減少模型對訓練數據細節的依賴，確保生成的嵌入能泛化到新的提問。
* 語義匹配：聊天機器人需要將用戶的提問（可能有語法或措辭差異）映射到正確的連結。Dropout 幫助模型學習更魯棒的語義特徵，應對提問的多樣性。
* 穩定性：Dropout 與推薦的低學習率（lr=2e-5）和 Warmup Steps（50）一起，確保訓練過程穩定，特別是在小數據集的早期階段。

