## Monitor
![image](image/llm.png)
AI Agentå¯ä»¥è¨­å®šä¸åŒToolsä¾†å°ä¸åŒçš„è³‡æ–™æºåšè³‡æ–™æœå°‹ï¼Œå¯ä»¥å‘¼å«APIã€å¯ä»¥æœå°‹Vector DB

å…ˆå®šç¾©å¥½Tools
```
@tool
def get_order_info(order_id: str) -> dict:
    """é€é API å–å¾—æŒ‡å®šè¨‚å–®çš„è³‡è¨Šï¼Œä¾‹å¦‚è¨‚å–®ç‹€æ…‹å’Œè¿½è¹¤è™Ÿã€‚"""
    print('order_id: ', order_id)
    return {"status": "shipped", "tracking": "ABC123"}

@tool
def get_inventory(product_id: str) -> dict:
    """æŸ¥è©¢æŒ‡å®šå•†å“çš„åº«å­˜æ•¸é‡å’Œå¯ç”¨æ€§ã€‚"""
    print('product_id: ', product_id)
    return {"stock": 50, "available": True}

@tool
def rag_retrieve(query: str) -> str:
    """å¾çŸ¥è­˜åº«æª¢ç´¢èˆ‡æŸ¥è©¢ç›¸é—œçš„è³‡è¨Šã€‚"""
    query_docs = retriever.get_relevant_documents(query)
    return "\n".join([doc.page_content for doc in query_docs])

```

å¯ä»¥é€éAgentå°‡LLM Serverè·ŸToolsç¶å®š
```
llm = ChatOllama(
    model="llama3.1",  # æ”¹ç”¨æ”¯æ´å·¥å…·èª¿ç”¨çš„æ¨¡å‹
    base_url="http://localhost:11434"
)
tools = [get_order_info, get_inventory, rag_retrieve]
agent = create_react_agent(llm, tools)
```

å»ºç½®System Messageå®šç¾©å“ªäº›æƒ…å¢ƒç”¨å“ªäº›Tool
```
system_message = SystemMessage(content="""ä½ æ˜¯é›»å•†å®¢æœï¼Œå›ç­”ç°¡æ½”ä¸”å°ˆæ¥­ã€‚
æ­¥é©Ÿ1: è§£æç”¨æˆ¶æŸ¥è©¢ï¼ˆè¨‚å–®ã€åº«å­˜ã€å…¶ä»–ï¼‰ã€‚
æ­¥é©Ÿ2: ä½¿ç”¨é©ç•¶å·¥å…·ï¼ˆget_order_infoã€get_inventoryã€rag_retrieveï¼‰ç²å–è³‡æ–™ã€‚
æ­¥é©Ÿ3: åˆæˆå‹å–„çš„è‡ªç„¶èªè¨€å›æ‡‰ã€‚""")
```

è·Ÿagentæå•å•é¡Œ
```
question = input("ğŸ” å•é¡Œ / Question: ")
if question.lower() in {"exit", "quit"}:
  break
messages = [system_message, HumanMessage(content=question)]
response = agent.invoke({"messages": messages})
print("\nğŸ“£ å›ç­” / Answer:\n", response["messages"][-1].content)
```

## Model
* Embedding Model
* Tool Enabled LLM 

## Service
* Vector DB
  * FASSIS
  * Milvus

* LLM Server
  * Ollama